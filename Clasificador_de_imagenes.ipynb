{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "isVexGTge9TS"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.19.0\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPUs disponibles: []\n"
          ]
        }
      ],
      "source": [
        "# Si ves una lista con al menos un dispositivo GPU, TensorFlow la está detectando.\n",
        "# Si la lista está vacía ([]), TensorFlow no está usando la GPU.\n",
        "print(\"GPUs disponibles:\", tf.config.list_physical_devices('GPU'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xxjQOdsSHuTe"
      },
      "source": [
        "Importación de librerias."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "IvJTJlJufjoo"
      },
      "outputs": [],
      "source": [
        "(imagenes_entrenamiento, etiquetas_entrenamiento ), (imagenes_verificacion, etiquetas_verificacion) = keras.datasets.cifar10.load_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H4B4wxwrHxzU"
      },
      "source": [
        "Carga de conjunto de datos de imagenes para entrenamiento. 50.000 fotos en color de 32x32 pixels de aviones, trenes, autos, etc con etiquetas. 10000 fotos para testear"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DPtPXU1Ih4uJ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kgSAiPLKgJup"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(50000, 1) (50000, 32, 32, 3)\n"
          ]
        }
      ],
      "source": [
        "print(etiquetas_entrenamiento.shape, imagenes_entrenamiento.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WKfx9chMGpLw"
      },
      "outputs": [],
      "source": [
        "imagenes_entrenamiento, imagenes_verificacion = imagenes_entrenamiento / 255.0, imagenes_verificacion  / 255.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-joGocTXHGJY"
      },
      "source": [
        "AGREGADO: Estandarizacion entre 0s y 1s"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UE3X9-ediNZS"
      },
      "outputs": [],
      "source": [
        "nombres_clases = ['airplane','automobile','bird','cat','deer',\n",
        "                  'dog','frog','horse','ship','truck']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "63EJWX7eFvLx"
      },
      "source": [
        "Funcion para mostrar imagenes en miniatura."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Knj4hxtjBy3"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "def mostrar():\n",
        "  plt.figure(figsize=(5,5))\n",
        "  for i in range(10):\n",
        "    plt.subplot(4,4,i+1)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.grid(False)\n",
        "    plt.imshow(imagenes_entrenamiento[i], cmap=plt.cm.binary)\n",
        "    plt.xlabel(nombres_clases[etiquetas_entrenamiento[i][0]])\n",
        "  plt.show()\n",
        "mostrar()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pUdR_mDdF2oB"
      },
      "source": [
        "Una capa de entrada para imagenes de 32x32 pixels con 3 dimensiones de colores.\n",
        "Dos capas convolucionales para extraer caracteristicas, con 32 filtros de 3x3. Quitando relleno que no se ajusta.\n",
        "Dos capas de agrupamiento con filtros de 2x2 para achicar la anterior.\n",
        "\n",
        "Una capa complemtamente conectada (densa) con 64 neuronas.\n",
        "Una capa de salida con 10 neuronas para clasificar."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kQmkPf2SlhWA"
      },
      "outputs": [],
      "source": [
        "capa = keras.layers\n",
        "modelo = keras.models.Sequential()\n",
        "modelo.add(capa.Conv2D(64, (3,3), padding=\"same\", activation='relu', input_shape=(32,32,3)))\n",
        "modelo.add(capa.BatchNormalization())\n",
        "modelo.add(capa.Conv2D(64, (3,3), padding=\"same\", activation='relu')) # capa extra\n",
        "modelo.add(capa.BatchNormalization()) \n",
        "modelo.add(capa.MaxPool2D(2,2))\n",
        "modelo.add(capa.Dropout(0.3)) # ajusto de 0.25 a 0.3\n",
        "\n",
        "modelo.add(capa.Conv2D(128, (3,3), padding=\"same\", activation='relu')) # aumento el número de filtros de 64 a 128\n",
        "modelo.add(capa.BatchNormalization())\n",
        "modelo.add(capa.Conv2D(128, (3,3), padding=\"same\", activation='relu')) # capa extra\n",
        "modelo.add(capa.BatchNormalization())\n",
        "modelo.add(capa.MaxPool2D(2,2))\n",
        "modelo.add(capa.Dropout(0.3)) # ajusto de 0.25 a 0.3\n",
        "\n",
        "modelo.add(capa.Flatten())\n",
        "modelo.add(capa.Dense(128, activation='relu', kernel_regularizer=keras.regularizers.l2(0.001))) # aumento el número de neuronas de 64 a 128\n",
        "modelo.add(capa.Dropout(0.5))\n",
        "modelo.add(capa.Dense(10, activation='softmax')) # agrego softmax para clasificacion multiclase \n",
        "\n",
        "modelo.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z3xT_ODfHEMD"
      },
      "source": [
        "Configuracion del modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lzUK64xJqrmn"
      },
      "outputs": [],
      "source": [
        "modelo.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R8OBkI-wHGki"
      },
      "source": [
        "Entrenamiento (creación del modelo)\n",
        "\n",
        "Loss se busca el número mas bajo. Accuracy el más alto"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
            "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
            "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
            "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
            "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
            "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
            "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
            "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
            "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
            "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
            "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
            "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
            "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
            "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
            "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
            "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
            "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
            "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
            "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
            "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
            "Epoch 1/100\n",
            "196/196 [==============================] - 14s 62ms/step - loss: 2.3030 - accuracy: 0.2705 - val_loss: 3.1599 - val_accuracy: 0.1668 - lr: 0.0010\n",
            "Epoch 2/100\n",
            "196/196 [==============================] - 12s 60ms/step - loss: 1.9151 - accuracy: 0.3628 - val_loss: 2.6215 - val_accuracy: 0.2094 - lr: 0.0010\n",
            "Epoch 3/100\n",
            "196/196 [==============================] - 12s 59ms/step - loss: 1.7209 - accuracy: 0.4238 - val_loss: 1.3960 - val_accuracy: 0.5719 - lr: 0.0010\n",
            "Epoch 4/100\n",
            "196/196 [==============================] - 12s 58ms/step - loss: 1.5949 - accuracy: 0.4638 - val_loss: 1.2831 - val_accuracy: 0.5849 - lr: 0.0010\n",
            "Epoch 5/100\n",
            "196/196 [==============================] - 12s 56ms/step - loss: 1.5077 - accuracy: 0.4961 - val_loss: 1.4381 - val_accuracy: 0.5383 - lr: 0.0010\n",
            "Epoch 6/100\n",
            "196/196 [==============================] - 12s 60ms/step - loss: 1.4274 - accuracy: 0.5281 - val_loss: 1.2399 - val_accuracy: 0.6140 - lr: 0.0010\n",
            "Epoch 7/100\n",
            "196/196 [==============================] - 12s 57ms/step - loss: 1.3846 - accuracy: 0.5568 - val_loss: 1.3565 - val_accuracy: 0.5815 - lr: 0.0010\n",
            "Epoch 8/100\n",
            "196/196 [==============================] - 12s 59ms/step - loss: 1.3392 - accuracy: 0.5819 - val_loss: 1.2074 - val_accuracy: 0.6339 - lr: 0.0010\n",
            "Epoch 9/100\n",
            "196/196 [==============================] - 12s 59ms/step - loss: 1.2913 - accuracy: 0.6090 - val_loss: 1.1644 - val_accuracy: 0.6528 - lr: 0.0010\n",
            "Epoch 10/100\n",
            "196/196 [==============================] - 12s 60ms/step - loss: 1.2599 - accuracy: 0.6289 - val_loss: 1.1627 - val_accuracy: 0.6758 - lr: 0.0010\n",
            "Epoch 11/100\n",
            "196/196 [==============================] - 12s 57ms/step - loss: 1.2346 - accuracy: 0.6493 - val_loss: 1.2347 - val_accuracy: 0.6496 - lr: 0.0010\n",
            "Epoch 12/100\n",
            "196/196 [==============================] - 13s 62ms/step - loss: 1.1992 - accuracy: 0.6661 - val_loss: 1.1483 - val_accuracy: 0.6884 - lr: 0.0010\n",
            "Epoch 13/100\n",
            "196/196 [==============================] - 12s 59ms/step - loss: 1.1662 - accuracy: 0.6824 - val_loss: 1.0629 - val_accuracy: 0.7253 - lr: 0.0010\n",
            "Epoch 14/100\n",
            "196/196 [==============================] - 12s 58ms/step - loss: 1.1553 - accuracy: 0.6931 - val_loss: 1.4469 - val_accuracy: 0.6448 - lr: 0.0010\n",
            "Epoch 15/100\n",
            "196/196 [==============================] - 12s 58ms/step - loss: 1.1368 - accuracy: 0.7009 - val_loss: 1.1212 - val_accuracy: 0.7134 - lr: 0.0010\n",
            "Epoch 16/100\n",
            "196/196 [==============================] - 12s 60ms/step - loss: 1.1146 - accuracy: 0.7113 - val_loss: 0.9954 - val_accuracy: 0.7545 - lr: 0.0010\n",
            "Epoch 17/100\n",
            "196/196 [==============================] - 12s 59ms/step - loss: 1.0897 - accuracy: 0.7223 - val_loss: 1.0524 - val_accuracy: 0.7428 - lr: 0.0010\n",
            "Epoch 18/100\n",
            "196/196 [==============================] - 12s 58ms/step - loss: 1.0832 - accuracy: 0.7243 - val_loss: 1.0861 - val_accuracy: 0.7272 - lr: 0.0010\n",
            "Epoch 19/100\n",
            "196/196 [==============================] - 12s 57ms/step - loss: 1.0577 - accuracy: 0.7361 - val_loss: 1.2750 - val_accuracy: 0.6811 - lr: 0.0010\n",
            "Epoch 20/100\n",
            "196/196 [==============================] - 12s 58ms/step - loss: 1.0394 - accuracy: 0.7423 - val_loss: 1.0424 - val_accuracy: 0.7447 - lr: 0.0010\n",
            "Epoch 21/100\n",
            "196/196 [==============================] - 12s 59ms/step - loss: 1.0283 - accuracy: 0.7454 - val_loss: 0.9971 - val_accuracy: 0.7571 - lr: 0.0010\n",
            "Epoch 22/100\n",
            "196/196 [==============================] - 13s 61ms/step - loss: 0.9312 - accuracy: 0.7749 - val_loss: 0.8768 - val_accuracy: 0.7859 - lr: 2.0000e-04\n",
            "Epoch 23/100\n",
            "196/196 [==============================] - 12s 60ms/step - loss: 0.8697 - accuracy: 0.7859 - val_loss: 0.8368 - val_accuracy: 0.7922 - lr: 2.0000e-04\n",
            "Epoch 24/100\n",
            "196/196 [==============================] - 12s 59ms/step - loss: 0.8314 - accuracy: 0.7921 - val_loss: 0.8174 - val_accuracy: 0.7952 - lr: 2.0000e-04\n",
            "Epoch 25/100\n",
            "196/196 [==============================] - 12s 60ms/step - loss: 0.7972 - accuracy: 0.7961 - val_loss: 0.7979 - val_accuracy: 0.7996 - lr: 2.0000e-04\n",
            "Epoch 26/100\n",
            "196/196 [==============================] - 12s 60ms/step - loss: 0.7718 - accuracy: 0.8000 - val_loss: 0.7591 - val_accuracy: 0.8035 - lr: 2.0000e-04\n",
            "Epoch 27/100\n",
            "196/196 [==============================] - 13s 61ms/step - loss: 0.7504 - accuracy: 0.8028 - val_loss: 0.7434 - val_accuracy: 0.8064 - lr: 2.0000e-04\n",
            "Epoch 28/100\n",
            "196/196 [==============================] - 13s 61ms/step - loss: 0.7302 - accuracy: 0.8066 - val_loss: 0.7272 - val_accuracy: 0.8092 - lr: 2.0000e-04\n",
            "Epoch 29/100\n",
            "196/196 [==============================] - 13s 63ms/step - loss: 0.7125 - accuracy: 0.8089 - val_loss: 0.7127 - val_accuracy: 0.8118 - lr: 2.0000e-04\n",
            "Epoch 30/100\n",
            "196/196 [==============================] - 12s 59ms/step - loss: 0.7037 - accuracy: 0.8113 - val_loss: 0.7233 - val_accuracy: 0.8056 - lr: 2.0000e-04\n",
            "Epoch 31/100\n",
            "196/196 [==============================] - 12s 60ms/step - loss: 0.6934 - accuracy: 0.8107 - val_loss: 0.6984 - val_accuracy: 0.8128 - lr: 2.0000e-04\n",
            "Epoch 32/100\n",
            "196/196 [==============================] - 12s 56ms/step - loss: 0.6798 - accuracy: 0.8132 - val_loss: 0.7157 - val_accuracy: 0.8045 - lr: 2.0000e-04\n",
            "Epoch 33/100\n",
            "196/196 [==============================] - 12s 58ms/step - loss: 0.6607 - accuracy: 0.8183 - val_loss: 0.6744 - val_accuracy: 0.8153 - lr: 2.0000e-04\n",
            "Epoch 34/100\n",
            "196/196 [==============================] - 12s 59ms/step - loss: 0.6548 - accuracy: 0.8185 - val_loss: 0.6859 - val_accuracy: 0.8141 - lr: 2.0000e-04\n",
            "Epoch 35/100\n",
            "196/196 [==============================] - 12s 57ms/step - loss: 0.6501 - accuracy: 0.8194 - val_loss: 0.6926 - val_accuracy: 0.8097 - lr: 2.0000e-04\n",
            "Epoch 36/100\n",
            "196/196 [==============================] - 12s 59ms/step - loss: 0.6399 - accuracy: 0.8245 - val_loss: 0.6630 - val_accuracy: 0.8182 - lr: 2.0000e-04\n",
            "Epoch 37/100\n",
            "196/196 [==============================] - 12s 58ms/step - loss: 0.6376 - accuracy: 0.8236 - val_loss: 0.6813 - val_accuracy: 0.8159 - lr: 2.0000e-04\n",
            "Epoch 38/100\n",
            "196/196 [==============================] - 12s 59ms/step - loss: 0.6286 - accuracy: 0.8235 - val_loss: 0.6780 - val_accuracy: 0.8181 - lr: 2.0000e-04\n",
            "Epoch 39/100\n",
            "196/196 [==============================] - 12s 58ms/step - loss: 0.6256 - accuracy: 0.8270 - val_loss: 0.6796 - val_accuracy: 0.8111 - lr: 2.0000e-04\n",
            "Epoch 40/100\n",
            "196/196 [==============================] - 12s 59ms/step - loss: 0.6186 - accuracy: 0.8261 - val_loss: 0.6645 - val_accuracy: 0.8179 - lr: 2.0000e-04\n",
            "Epoch 41/100\n",
            "196/196 [==============================] - 12s 59ms/step - loss: 0.6126 - accuracy: 0.8295 - val_loss: 0.6754 - val_accuracy: 0.8123 - lr: 2.0000e-04\n",
            "Epoch 42/100\n",
            "196/196 [==============================] - 13s 61ms/step - loss: 0.5934 - accuracy: 0.8369 - val_loss: 0.6617 - val_accuracy: 0.8205 - lr: 4.0000e-05\n",
            "Epoch 43/100\n",
            "196/196 [==============================] - 12s 58ms/step - loss: 0.5869 - accuracy: 0.8366 - val_loss: 0.6566 - val_accuracy: 0.8205 - lr: 4.0000e-05\n",
            "Epoch 44/100\n",
            "196/196 [==============================] - 12s 60ms/step - loss: 0.5742 - accuracy: 0.8393 - val_loss: 0.6559 - val_accuracy: 0.8211 - lr: 4.0000e-05\n",
            "Epoch 45/100\n",
            "196/196 [==============================] - 12s 60ms/step - loss: 0.5776 - accuracy: 0.8408 - val_loss: 0.6499 - val_accuracy: 0.8217 - lr: 4.0000e-05\n",
            "Epoch 46/100\n",
            "196/196 [==============================] - 12s 59ms/step - loss: 0.5713 - accuracy: 0.8412 - val_loss: 0.6453 - val_accuracy: 0.8241 - lr: 4.0000e-05\n",
            "Epoch 47/100\n",
            "196/196 [==============================] - 12s 60ms/step - loss: 0.5656 - accuracy: 0.8433 - val_loss: 0.6366 - val_accuracy: 0.8264 - lr: 4.0000e-05\n",
            "Epoch 48/100\n",
            "196/196 [==============================] - 12s 58ms/step - loss: 0.5630 - accuracy: 0.8442 - val_loss: 0.6394 - val_accuracy: 0.8247 - lr: 4.0000e-05\n",
            "Epoch 49/100\n",
            "196/196 [==============================] - 12s 58ms/step - loss: 0.5572 - accuracy: 0.8449 - val_loss: 0.6376 - val_accuracy: 0.8233 - lr: 4.0000e-05\n",
            "Epoch 50/100\n",
            "196/196 [==============================] - 12s 59ms/step - loss: 0.5591 - accuracy: 0.8428 - val_loss: 0.6332 - val_accuracy: 0.8272 - lr: 4.0000e-05\n",
            "Epoch 51/100\n",
            "196/196 [==============================] - 12s 58ms/step - loss: 0.5529 - accuracy: 0.8463 - val_loss: 0.6346 - val_accuracy: 0.8260 - lr: 4.0000e-05\n",
            "Epoch 52/100\n",
            "196/196 [==============================] - 12s 58ms/step - loss: 0.5504 - accuracy: 0.8439 - val_loss: 0.6390 - val_accuracy: 0.8241 - lr: 4.0000e-05\n",
            "Epoch 53/100\n",
            "196/196 [==============================] - 12s 58ms/step - loss: 0.5519 - accuracy: 0.8445 - val_loss: 0.6346 - val_accuracy: 0.8238 - lr: 4.0000e-05\n",
            "Epoch 54/100\n",
            "196/196 [==============================] - 12s 58ms/step - loss: 0.5442 - accuracy: 0.8470 - val_loss: 0.6318 - val_accuracy: 0.8250 - lr: 4.0000e-05\n",
            "Epoch 55/100\n",
            "196/196 [==============================] - 12s 60ms/step - loss: 0.5404 - accuracy: 0.8478 - val_loss: 0.6292 - val_accuracy: 0.8291 - lr: 4.0000e-05\n",
            "Epoch 56/100\n",
            "196/196 [==============================] - 12s 58ms/step - loss: 0.5381 - accuracy: 0.8480 - val_loss: 0.6283 - val_accuracy: 0.8254 - lr: 4.0000e-05\n",
            "Epoch 57/100\n",
            "196/196 [==============================] - 12s 59ms/step - loss: 0.5364 - accuracy: 0.8484 - val_loss: 0.6262 - val_accuracy: 0.8271 - lr: 4.0000e-05\n",
            "Epoch 58/100\n",
            "196/196 [==============================] - 12s 58ms/step - loss: 0.5313 - accuracy: 0.8512 - val_loss: 0.6242 - val_accuracy: 0.8279 - lr: 4.0000e-05\n",
            "Epoch 59/100\n",
            "196/196 [==============================] - 12s 57ms/step - loss: 0.5365 - accuracy: 0.8476 - val_loss: 0.6101 - val_accuracy: 0.8291 - lr: 4.0000e-05\n",
            "Epoch 60/100\n",
            "196/196 [==============================] - 12s 58ms/step - loss: 0.5262 - accuracy: 0.8513 - val_loss: 0.6125 - val_accuracy: 0.8290 - lr: 4.0000e-05\n",
            "Epoch 61/100\n",
            "196/196 [==============================] - 12s 57ms/step - loss: 0.5310 - accuracy: 0.8510 - val_loss: 0.6214 - val_accuracy: 0.8250 - lr: 4.0000e-05\n",
            "Epoch 62/100\n",
            "196/196 [==============================] - 12s 58ms/step - loss: 0.5182 - accuracy: 0.8526 - val_loss: 0.6159 - val_accuracy: 0.8279 - lr: 4.0000e-05\n",
            "Epoch 63/100\n",
            "196/196 [==============================] - 12s 59ms/step - loss: 0.5225 - accuracy: 0.8517 - val_loss: 0.6134 - val_accuracy: 0.8284 - lr: 4.0000e-05\n",
            "Epoch 64/100\n",
            "196/196 [==============================] - 12s 58ms/step - loss: 0.5186 - accuracy: 0.8532 - val_loss: 0.6138 - val_accuracy: 0.8290 - lr: 4.0000e-05\n",
            "Epoch 65/100\n",
            "196/196 [==============================] - 12s 60ms/step - loss: 0.5187 - accuracy: 0.8523 - val_loss: 0.6133 - val_accuracy: 0.8302 - lr: 1.0000e-05\n",
            "Epoch 66/100\n",
            "196/196 [==============================] - 12s 57ms/step - loss: 0.5212 - accuracy: 0.8519 - val_loss: 0.6128 - val_accuracy: 0.8300 - lr: 1.0000e-05\n",
            "Epoch 67/100\n",
            "196/196 [==============================] - 12s 58ms/step - loss: 0.5146 - accuracy: 0.8548 - val_loss: 0.6139 - val_accuracy: 0.8291 - lr: 1.0000e-05\n",
            "Epoch 68/100\n",
            "196/196 [==============================] - 12s 57ms/step - loss: 0.5141 - accuracy: 0.8536 - val_loss: 0.6113 - val_accuracy: 0.8298 - lr: 1.0000e-05\n",
            "Epoch 69/100\n",
            "196/196 [==============================] - 12s 59ms/step - loss: 0.5120 - accuracy: 0.8543 - val_loss: 0.6114 - val_accuracy: 0.8300 - lr: 1.0000e-05\n"
          ]
        }
      ],
      "source": [
        "# Callbacks para optimizar el entrenamiento\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
        "import tensorflow as tf\n",
        "\n",
        "# 1. Crear una pipeline de datos eficiente con tf.data\n",
        "# Capas para hacer Data Augmentation en la GPU (mucho más rápido)\n",
        "data_augmentation_layers = tf.keras.Sequential([\n",
        "    tf.keras.layers.RandomFlip(\"horizontal\"),\n",
        "    tf.keras.layers.RandomRotation(0.1),\n",
        "    tf.keras.layers.RandomZoom(0.1),\n",
        "])\n",
        "\n",
        "# Crear los datasets\n",
        "batch_size = 256 # Un batch size más grande es mejor para la GPU\n",
        "autotune = tf.data.AUTOTUNE # Permite a TensorFlow optimizar la pipeline automáticamente\n",
        "\n",
        "# Dataset de entrenamiento\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((imagenes_entrenamiento, etiquetas_entrenamiento))\n",
        "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(batch_size)\n",
        "train_dataset = train_dataset.map(lambda x, y: (data_augmentation_layers(x, training=True), y), num_parallel_calls=autotune)\n",
        "train_dataset = train_dataset.prefetch(buffer_size=autotune) # ¡Esta es la clave!\n",
        "\n",
        "# Dataset de validación\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices((imagenes_verificacion, etiquetas_verificacion))\n",
        "val_dataset = val_dataset.batch(batch_size).prefetch(buffer_size=autotune)\n",
        "\n",
        "\n",
        "# 2. Definición de Callbacks\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "checkpoint = ModelCheckpoint('mejor_modelo.keras', monitor='val_accuracy', save_best_only=True, mode='max')\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.00001)\n",
        "\n",
        "# 3. Entrenamiento del modelo usando la nueva pipeline de datos\n",
        "history = modelo.fit(\n",
        "    train_dataset,\n",
        "    epochs=100,\n",
        "    validation_data=val_dataset,\n",
        "    callbacks=[early_stop, checkpoint, reduce_lr]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OkWvOKlGtTNI"
      },
      "outputs": [],
      "source": [
        "modelo.evaluate(imagenes_verificacion, etiquetas_verificacion, batch_size=256)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4MOZNMvhwOVF"
      },
      "outputs": [],
      "source": [
        "# predict()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XEvcjynLLdK6"
      },
      "source": [
        "**CONTINUAR DESDE ACA**"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
